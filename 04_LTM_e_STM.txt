1) State management: Come gestire lo stato di un agente che deve sopravvivere a decine di step senza perdersi - questo riguarda la capacità di mantenere coerenza e contesto durante operazioni complesse e lunghe
2) Planning: Come implementare una strategia che non collassi dopo il terzo tool call - si riferisce alla difficoltà di creare agenti che possano pianificare sequenze di azioni complesse senza degradare le prestazioni
3) Multi-agent orchestration: Come far dialogare più agenti LLM tra loro senza che si pestino i piedi o entrino in loop infiniti - affronta il problema della coordinazione tra agenti multipli evitando loop infiniti e conflitti
4) Long-term memory: Come gestire il contesto a lungo termine senza esplodere di token e di costi - la sfida di mantenere memoria persistente in modo efficiente dal punto di vista computazionale ed economico
5) Error recovery: Cosa succede quando un agente fallisce a metà di un workflow complesso - riguarda la resilienza dei sistemi agentici e la capacità di recuperare da errori senza compromettere l'intero processo

✅ 4. Long-term memory — Risolto completamente
- È il problema che abbiamo affrontato direttamente:
- Mem0 persiste fatti cross-sessione senza salvare tutta la conversazione
- Token cost controllato: solo fatti atomici estratti, non testo grezzo
- Retrieval on-demand: top-K fatti rilevanti per query, non dump completo
- Costo: ~200 token per retrieve, ~500 per save — scalabile

✅ 1. State management — Parzialmente risolto
- STM sliding window garantisce coerenza nei turni recenti
- LTM garantisce persistenza dei fatti chiave cross-sessione
- Limite: non gestiamo ancora stato di workflow complessi (es. "task al 60% completato")


**Soluzione**

FLUSSO STM e LTM

INPUT utente
    ↓
prune_memory()          → STM sliding window (turn-based, SOTA)
    ↓ (se > max_turns)
save_to_ltm()           → salva fatti scaduti in Mem0
    ↓
get_ltm_context_async() → Mem0 search top-K fatti rilevanti
    ↓
orchestrator.system_prompt += LTM  → inject diretto nell'agente
    ↓
stream_invoke()         → multi-agent execution
    ↓
save_final_response_to_ltm_async() → salva fatti del turno corrente
    ↓
OUTPUT risposta personalizzata
